# Databricks Workspace Inventory Configuration
# 
# Copy this file to .env or set these as environment variables
# DO NOT commit actual tokens to version control!

# ============================================================================
# Required Configuration
# ============================================================================

# Your Databricks workspace URL
# Format: https://your-workspace.cloud.databricks.com
DATABRICKS_WORKSPACE_URL=https://your-workspace.cloud.databricks.com

# Your Databricks Personal Access Token
# Generate at: User Settings -> Access Tokens
DATABRICKS_TOKEN=your-token-here

# ============================================================================
# Spark Connect Configuration (for local execution)
# ============================================================================

# Enable Spark Connect mode (set to "true" for local execution)
# Default: "false" (auto-detected if DATABRICKS_CLUSTER_ID or DATABRICKS_SERVERLESS_COMPUTE_ID is set)
USE_SPARK_CONNECT=false

# Option 1: Traditional Cluster - Cluster ID to connect to via Spark Connect
# Find it in the cluster URL or use: databricks clusters list
DATABRICKS_CLUSTER_ID=your-cluster-id-here

# Option 2: Serverless Compute - Set to "auto" to use serverless compute
# Requires: databricks-connect package installed
# Note: Serverless compute automatically scales and manages infrastructure
DATABRICKS_SERVERLESS_COMPUTE_ID=auto

# Databricks CLI Profile (optional)
# If you have multiple profiles in ~/.databrickscfg, specify which one to use
# DATABRICKS_CONFIG_PROFILE=DEFAULT

# ============================================================================
# Output Configuration
# ============================================================================

# Output path for the inventory CSV file
# Default: /tmp/workspace_inventory
# For DBFS storage, use: /dbfs/workspace_inventory
OUTPUT_PATH=/tmp/workspace_inventory

# Output format (csv, parquet, or delta)
# Default: csv
OUTPUT_FORMAT=csv

